{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project of deep learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents the pre-processing part of our project. By utilizing the Free Music Archive (FMA) dataset, we process audio tracks into spectrograms using the Librosa library and create a structured dataset. The notebook covers environment setup, data preprocessing, and preparing the data for model training and evaluation. Detailed explanations and code for converting audio to spectrograms, organizing the data, knowing its structure and contents are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and installs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the code you have to create a local environment with conda and activate it. The provided environment.yml file has all the required dependencies. Run the following command: \n",
    "- conda env create --file environment.yml  \n",
    "\n",
    "to create a conda environment with all the required dependencies and then activate it:\n",
    "- conda activate xnap-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#!pip install librosa numpy matplotlib pandas torch\n",
    "import os\n",
    "import librosa\n",
    "import math\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import ast\n",
    "import zipfile\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (24.0)\n",
      "Requirement already satisfied: setuptools in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (70.0.0)\n",
      "Requirement already satisfied: wheel in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.43.0)\n",
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip setuptools wheel\n",
    "#!pip install numpy==1.12.1  # workaround resampy's bogus setup.py\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'fma'...\n",
      "remote: Enumerating objects: 823, done.\u001b[K\n",
      "remote: Counting objects: 100% (822/822), done.\u001b[K\n",
      "remote: Compressing objects: 100% (287/287), done.\u001b[K\n",
      "remote: Total 823 (delta 532), reused 621 (delta 527), pack-reused 1\u001b[K\n",
      "Receiving objects: 100% (823/823), 4.08 MiB | 27.11 MiB/s, done.\n",
      "Resolving deltas: 100% (532/532), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mdeff/fma.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma\n"
     ]
    }
   ],
   "source": [
    "cd fma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install -y p7zip-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'fma'\n",
      "/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  341M  100  341M    0     0   120M      0  0:00:02  0:00:02 --:--:--  120M\n",
      "fma_metadata.zip: OK\n"
     ]
    }
   ],
   "source": [
    "# Change directory to 'fma'\n",
    "%cd fma\n",
    "\n",
    "# Download the datasets\n",
    "!curl -O https://os.unil.cloud.switch.ch/fma/fma_small.zip \n",
    "!curl -O https://os.unil.cloud.switch.ch/fma/fma_metadata.zip\n",
    "\n",
    "# Verify the downloaded files\n",
    "!echo \"ade154f733639d52e35e32f5593efe5be76c6d70  fma_small.zip\"    | sha1sum -c -\n",
    "!echo \"f0df49ffe5f2a6008d7dc83c6915b31835dfe733  fma_metadata.zip\" | sha1sum -c -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,6 CPUs Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz (406F1),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "  0M Sca        1 file, 358412441 bytes (342 MiB)\n",
      "\n",
      "Extracting archive: fma_metadata.zip\n",
      "--\n",
      "Path = fma_metadata.zip\n",
      "Type = zip\n",
      "Physical Size = 358412441\n",
      "\n",
      "      0% 4 - fma_metadata/raw_albums.c                                    1% 4 - fma_metadata/raw_albums.c                                    2% 5 - fma_metadata/raw_artists.cs                                      3% 6 - fma_metadata/raw_tracks.c                                    4% 6 - fma_metadata/raw_tracks.c                                    5% 6 - fma_metadata/raw_tracks.c                                    6% 6 - fma_metadata/raw_tracks.c                                    7% 6 - fma_metadata/raw_tracks.c                                    8% 6 - fma_metadata/raw_tracks.c                                    9% 6 - fma_metadata/raw_tracks.c                                   10% 6 - fma_metadata/raw_tracks.c                                   11% 7 - fma_metadata/tracks.c                               12% 7 - fma_metadata/tracks.c                               13% 7 - fma_metadata/tracks.c                               14% 7 - fma_metadata/tracks.c                               15% 7 - fma_metadata/tracks.c                               16% 7 - fma_metadata/tracks.c                               17% 7 - fma_metadata/tracks.c                               18% 7 - fma_metadata/tracks.c                               19% 7 - fma_metadata/tracks.c                               20% 7 - fma_metadata/tracks.c                               21% 7 - fma_metadata/tracks.c                               22% 7 - fma_metadata/tracks.c                               23% 7 - fma_metadata/tracks.c                               24% 7 - fma_metadata/tracks.c                               25% 7 - fma_metadata/tracks.c                               26% 7 - fma_metadata/tracks.c                               27% 7 - fma_metadata/tracks.c                               28% 7 - fma_metadata/tracks.c                               28%     29% 9 - fma_metadata/raw_echonest.c                                     30% 9 - fma_metadata/raw_echonest.c                                     31% 9 - fma_metadata/raw_echonest.c                                     32% 9 - fma_metadata/raw_echonest.c                                     32% 10 - fma_metadata/echonest.cs                                   33% 10 - fma_metadata/echonest.cs                                   34% 10 - fma_metadata/echonest.cs                                   35% 10 - fma_metadata/echonest.cs                                   35% 11 - fma_metadata/features.cs                                   36% 11 - fma_metadata/features.cs                                   37% 11 - fma_metadata/features.cs                                   38% 11 - fma_metadata/features.cs                                   39% 11 - fma_metadata/features.cs                                   40% 11 - fma_metadata/features.cs                                   41% 11 - fma_metadata/features.cs                                   42% 11 - fma_metadata/features.cs                                   43% 11 - fma_metadata/features.cs                                   44% 11 - fma_metadata/features.cs                                   45% 11 - fma_metadata/features.cs                                   46% 11 - fma_metadata/features.cs                                   47% 11 - fma_metadata/features.cs                                   48% 11 - fma_metadata/features.cs                                   49% 11 - fma_metadata/features.cs                                   50% 11 - fma_metadata/features.cs                                   51% 11 - fma_metadata/features.cs                                   52% 11 - fma_metadata/features.cs                                   53% 11 - fma_metadata/features.cs                                   54% 11 - fma_metadata/features.cs                                   55% 11 - fma_metadata/features.cs                                   56% 11 - fma_metadata/features.cs                                   57% 11 - fma_metadata/features.cs                                   58% 11 - fma_metadata/features.cs                                   59% 11 - fma_metadata/features.cs                                   60% 11 - fma_metadata/features.cs                                   61% 11 - fma_metadata/features.cs                                   62% 11 - fma_metadata/features.cs                                   63% 11 - fma_metadata/features.cs                                   64% 11 - fma_metadata/features.cs                                   65% 11 - fma_metadata/features.cs                                   66% 11 - fma_metadata/features.cs                                   67% 11 - fma_metadata/features.cs                                   68% 11 - fma_metadata/features.cs                                   69% 11 - fma_metadata/features.cs                                   70% 11 - fma_metadata/features.cs                                   71% 11 - fma_metadata/features.cs                                   72% 11 - fma_metadata/features.cs                                   73% 11 - fma_metadata/features.cs                                   74% 11 - fma_metadata/features.cs                                   75% 11 - fma_metadata/features.cs                                   76% 11 - fma_metadata/features.cs                                   77% 11 - fma_metadata/features.cs                                   78% 11 - fma_metadata/features.cs                                   79% 11 - fma_metadata/features.cs                                   80% 11 - fma_metadata/features.cs                                   81% 11 - fma_metadata/features.cs                                   82% 11 - fma_metadata/features.cs                                   83% 11 - fma_metadata/features.cs                                   84% 11 - fma_metadata/features.cs                                   85% 11 - fma_metadata/features.cs                                   86% 11 - fma_metadata/features.cs                                   87% 11 - fma_metadata/features.cs                                   88% 11 - fma_metadata/features.cs                                   89% 11 - fma_metadata/features.cs                                   90% 11 - fma_metadata/features.cs                                   91% 11 - fma_metadata/features.cs                                   92% 11 - fma_metadata/features.cs                                   93% 11 - fma_metadata/features.cs                                   94% 11 - fma_metadata/features.cs                                   95% 11 - fma_metadata/features.cs                                   96% 11 - fma_metadata/features.cs                                   97% 11 - fma_metadata/features.cs                                   98% 11 - fma_metadata/features.cs                                   99% 11 - fma_metadata/features.cs                                  100% 11 - fma_metadata/features.cs                                  Everything is Ok\n",
      "\n",
      "Files: 12\n",
      "Size:       1464836461\n",
      "Compressed: 358412441\n"
     ]
    }
   ],
   "source": [
    "# Unzip the downloaded files without prompts\n",
    "!7z x -aoa fma_metadata.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!7z x -aoa fma_small.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to extract and work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_genre_from_genre_id(df, id):\n",
    "    track_with_id = df[df['genre_id'] == id]\n",
    "    genre = track_with_id['title'].iloc[0]  # Make sure to extract the first element\n",
    "    return genre\n",
    "\n",
    "def get_genre_from_track_id(df, id, df2):\n",
    "    '''\n",
    "    df: tracks dataframe\n",
    "    id: track id\n",
    "    df2: geners dataframe\n",
    "    '''\n",
    "    track_with_id = df[df['track_id'] == id]\n",
    "    top_genre = track_with_id['genre_top'].iloc[0]  # Extract the first element\n",
    "    genres_all = track_with_id['genres_all'].iloc[0]  # Extract the first element\n",
    "    genres_all = ast.literal_eval(genres_all)  # Convert the string representation of a list to a list\n",
    "    genres_all = [get_genre_from_genre_id(df2, genre_id) for genre_id in genres_all]\n",
    "    return top_genre, genres_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "## To take out the left 0, which the code cannot work with\n",
    "def remove_leading_zeros(number):\n",
    "    return int(str(number))\n",
    "\n",
    "# Example usage\n",
    "number_with_zeros_str = \"00002\"\n",
    "number_without_zeros = remove_leading_zeros(number_with_zeros_str)\n",
    "print(number_without_zeros)  # Output: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def CreateSpectrograms(load_path, save_path):\n",
    "    \"\"\"\n",
    "    Recursively loads each song from subdirectories and creates its Mel Spectrogram using librosa,\n",
    "    with a Fast Fourier Transform window of 2048 and a hop length of 512. It also saves the spectrogram\n",
    "    on the save defined path with the id as the name\n",
    "\n",
    "    Parameters:\n",
    "    - load_path: Path where the audio files to load are found.\n",
    "    - save_path: Path where the spectrograms are saved.\n",
    "    \"\"\"\n",
    "\n",
    "    load_path = Path(load_path)\n",
    "    save_path = Path(save_path)\n",
    "\n",
    "    # Recursive function to process all files in the directory and subdirectories>\n",
    "    def process_directory(directory):\n",
    "        for file in directory.iterdir():\n",
    "            if file.is_dir():\n",
    "                process_directory(file)  # Recursively process subdirectory\n",
    "            else:\n",
    "                id_track = str(file.stem)  # Use the stem (filename without suffix) as id\n",
    "                try:\n",
    "                    waveform, sample_rate = librosa.load(file, mono=True)  # Load in mono\n",
    "                    spec = librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_fft=2048, hop_length=512)\n",
    "\n",
    "                    # Plot and save the spectrogram as .png without axis\n",
    "                    plt.figure(figsize=(10, 4))\n",
    "                    librosa.display.specshow(librosa.power_to_db(spec, ref=np.max), y_axis=None, x_axis=None)\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(save_path / f\"{id_track}.png\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "                    plt.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file}: {e}\")\n",
    "                    pass\n",
    "\n",
    "    process_directory(load_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_data(load_path, df, df2):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - load_path: Path where the audio files to load are found.\n",
    "    - df: DataFrame containing track information.\n",
    "    - df2: DataFrame containing genre information.\n",
    "    \"\"\"\n",
    "    load_path = Path(load_path)\n",
    "    data = []\n",
    "\n",
    "    # Recursive function to process all files in the directory and subdirectories\n",
    "    def process_directory(directory):\n",
    "        for file in directory.iterdir():\n",
    "            if file.is_dir():\n",
    "                process_directory(file)  # Recursively process subdirectory\n",
    "            elif file.suffix == '.png':  # Check if file has .mp3 extension\n",
    "                # Extract the ID from the file path using the stem attribute\n",
    "                id = file.stem\n",
    "                id = remove_leading_zeros(id)\n",
    "                genre, genres_all = get_genre_from_track_id(df, id, df2)\n",
    "                genre_top = df[df['track_id'] == id]['genre_top'].values[0]\n",
    "                data.append({'ID': id, 'Genre': genre, 'Genres_all': genres_all, 'Genre_top': genre_top})\n",
    "\n",
    "    process_directory(load_path)\n",
    "\n",
    "    return pd.DataFrame(data)  # Return DataFrame containing ID, Genre, Genres_all, and Genre_top columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Load the CSV files into DataFrames\n",
    "features = pd.read_csv('fma_metadata/features.csv')\n",
    "echonest = pd.read_csv('fma_metadata/echonest.csv')\n",
    "artists = pd.read_csv('fma_metadata/raw_artists.csv')\n",
    "albums = pd.read_csv('fma_metadata/raw_albums.csv')\n",
    "genres = pd.read_csv('fma_metadata/genres.csv')\n",
    "tracks = pd.read_csv('fma_metadata/tracks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the header values\n",
    "new_header = tracks.iloc[0]\n",
    "new_header[0] = 'track_id'  #track_id is the second row. the rest of elements are in the first one\n",
    "\n",
    "tracks = tracks[2:] # remove the first two rows of the dataframe\n",
    "\n",
    "tracks.columns = new_header\n",
    "\n",
    "tracks['track_id'] = tracks['track_id'].astype(int)  # track_id column has both int and string, all mixed up. convert everything to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['track_id', 'comments', 'date_created', 'date_released', 'engineer',\n",
       "       'favorites', 'id', 'information', 'listens', 'producer', 'tags',\n",
       "       'title', 'tracks', 'type', 'active_year_begin', 'active_year_end',\n",
       "       'associated_labels', 'bio', 'comments', 'date_created', 'favorites',\n",
       "       'id', 'latitude', 'location', 'longitude', 'members', 'name',\n",
       "       'related_projects', 'tags', 'website', 'wikipedia_page', 'split',\n",
       "       'subset', 'bit_rate', 'comments', 'composer', 'date_created',\n",
       "       'date_recorded', 'duration', 'favorites', 'genre_top', 'genres',\n",
       "       'genres_all', 'information', 'interest', 'language_code', 'license',\n",
       "       'listens', 'lyricist', 'number', 'publisher', 'tags', 'title'],\n",
       "      dtype='object', name=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['genre_id', '#tracks', 'parent', 'title', 'top_level'], dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genres.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'tracks' dataframe comprises crucial details for each track, including 'release date', 'producer', 'duration', 'genre_top', 'genres', and 'genres_all'.\n",
    "\n",
    "Given the project's objective to classify .mp3 files based on track genre, the primary focus lies on 'genre_top'. This column denotes the main genre of the track. Additionally, 'genres' and 'genres_all' provide supplementary genre information, capturing multiple genres associated with each track. While 'genre_top' is utilized for model training, it was considered to use 'genres_all' for assessing the model performance, offering a broader perspective owing to the inclusion of multiple track genres. However, 'genres_all' contained more than 8 unique genres, complicating the task of assessing the model using those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to remove the files that are corrupted or give some kind of mistake. \n",
    "\n",
    "#The processing already handles corrupted files and removes them\n",
    "\n",
    "#os.remove(\"/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma/fma_small/099/099134.mp3\")\n",
    "#os.remove(\"/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma/fma_small/108/108925.mp3\")\n",
    "#os.remove(\"/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma/fma_small/133/133297.mp3\")\n",
    "\n",
    "#os.remove(\"/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma/fma_small/098/098569.mp3\")\n",
    "#os.remove(\"/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma/fma_small/098/098565.mp3\")\n",
    "#os.remove(\"/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma/fma_small/098/098567.mp3\")\n",
    "\n",
    "#os.remove(\"/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/fma/fma_small/checksums\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spectrograms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the save path\n",
    "save_path = 'spectrograms'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Call the CreateSpectrograms function\n",
    "CreateSpectrograms('fma_small', save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zip into 17 different folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is done because github can't upload large files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_and_delete_directory(dir_path, max_zip_size=100*1024*1024):\n",
    "    # Check if directory exists\n",
    "    if os.path.exists(dir_path):\n",
    "        # If it exists, create zip archives\n",
    "        base_name = os.path.basename(dir_path)\n",
    "        files = os.listdir(dir_path)\n",
    "        zip_file_number = 1\n",
    "        zip_file = zipfile.ZipFile(os.path.join(os.path.dirname(dir_path), f\"{base_name}_{zip_file_number}.zip\"), 'w', zipfile.ZIP_DEFLATED)\n",
    "        total_size = 0\n",
    "\n",
    "        for file in files:\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "\n",
    "            if total_size > max_zip_size:\n",
    "                zip_file.close()\n",
    "                zip_file_number += 1\n",
    "                zip_file = zipfile.ZipFile(os.path.join(os.path.dirname(dir_path), f\"{base_name}_{zip_file_number}.zip\"), 'w', zipfile.ZIP_DEFLATED)\n",
    "                total_size = os.path.getsize(file_path)\n",
    "\n",
    "            zip_file.write(file_path, arcname=file)\n",
    "\n",
    "        zip_file.close()\n",
    "\n",
    "        # Delete the original directory\n",
    "        shutil.rmtree(dir_path)\n",
    "\n",
    "zip_and_delete_directory('/home/xnmaster/deep-learning-project-2024-ai_nndl_g9/spectrograms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip for each use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip_files(zip_filepaths, dest_dir):\n",
    "    # Check if destination directory exists\n",
    "    if not os.path.exists(dest_dir):\n",
    "        # If not, create the directory\n",
    "        os.makedirs(dest_dir)\n",
    "\n",
    "    for zip_filepath in zip_filepaths:\n",
    "        # Check if zip file exists\n",
    "        if not os.path.exists(zip_filepath):\n",
    "            print(f\"Zip file {zip_filepath} does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "            zip_ref.extractall(dest_dir)\n",
    "\n",
    "# List of zip files to unzip\n",
    "zip_filepaths = [f'/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/spectrograms_{i}.zip' for i in range(1, 18)]  ###.azureml\n",
    "\n",
    "unzip_files(zip_filepaths, '/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/spectrograms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### GET THE DATA: track id and its corresponding genre(s)\n",
    "data = get_data('/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/spectrograms', tracks, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Genres_all</th>\n",
       "      <th>Genre_top</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146019</td>\n",
       "      <td>Instrumental</td>\n",
       "      <td>[Soundtrack, Instrumental]</td>\n",
       "      <td>Instrumental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24430</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "      <td>Hip-Hop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27612</td>\n",
       "      <td>International</td>\n",
       "      <td>[International, Europe, Flamenco, Spanish, Latin]</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115925</td>\n",
       "      <td>Folk</td>\n",
       "      <td>[Psych-Folk, Folk, Freak-Folk]</td>\n",
       "      <td>Folk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55233</td>\n",
       "      <td>International</td>\n",
       "      <td>[International, Reggae - Dub]</td>\n",
       "      <td>International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7513</th>\n",
       "      <td>115698</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>[Techno, House, Electronic]</td>\n",
       "      <td>Electronic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7514</th>\n",
       "      <td>62748</td>\n",
       "      <td>Pop</td>\n",
       "      <td>[Pop, Synth Pop]</td>\n",
       "      <td>Pop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>126670</td>\n",
       "      <td>Rock</td>\n",
       "      <td>[Indie-Rock, Rock]</td>\n",
       "      <td>Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7516</th>\n",
       "      <td>97841</td>\n",
       "      <td>Experimental</td>\n",
       "      <td>[Improv, Experimental]</td>\n",
       "      <td>Experimental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7517</th>\n",
       "      <td>114559</td>\n",
       "      <td>Electronic</td>\n",
       "      <td>[Downtempo, Trip-Hop, House, Electronic]</td>\n",
       "      <td>Electronic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7518 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID          Genre   \n",
       "0     146019   Instrumental  \\\n",
       "1      24430        Hip-Hop   \n",
       "2      27612  International   \n",
       "3     115925           Folk   \n",
       "4      55233  International   \n",
       "...      ...            ...   \n",
       "7513  115698     Electronic   \n",
       "7514   62748            Pop   \n",
       "7515  126670           Rock   \n",
       "7516   97841   Experimental   \n",
       "7517  114559     Electronic   \n",
       "\n",
       "                                             Genres_all      Genre_top  \n",
       "0                            [Soundtrack, Instrumental]   Instrumental  \n",
       "1                                             [Hip-Hop]        Hip-Hop  \n",
       "2     [International, Europe, Flamenco, Spanish, Latin]  International  \n",
       "3                        [Psych-Folk, Folk, Freak-Folk]           Folk  \n",
       "4                         [International, Reggae - Dub]  International  \n",
       "...                                                 ...            ...  \n",
       "7513                        [Techno, House, Electronic]     Electronic  \n",
       "7514                                   [Pop, Synth Pop]            Pop  \n",
       "7515                                 [Indie-Rock, Rock]           Rock  \n",
       "7516                             [Improv, Experimental]   Experimental  \n",
       "7517           [Downtempo, Trip-Hop, House, Electronic]     Electronic  \n",
       "\n",
       "[7518 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "are_columns_equal = data['Genre'].equals(data['Genre_top'])\n",
    "print(are_columns_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values('ID')\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Genres_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>Pop</td>\n",
       "      <td>[Pop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140</td>\n",
       "      <td>Folk</td>\n",
       "      <td>[Folk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141</td>\n",
       "      <td>Folk</td>\n",
       "      <td>[Folk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7989</th>\n",
       "      <td>154308</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Hip-Hop Beats, Rap, Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>154309</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Hip-Hop Beats, Rap, Hip-Hop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7991</th>\n",
       "      <td>154413</td>\n",
       "      <td>Pop</td>\n",
       "      <td>[Pop, Experimental Pop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7992</th>\n",
       "      <td>154414</td>\n",
       "      <td>Pop</td>\n",
       "      <td>[Pop, Experimental Pop]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7993</th>\n",
       "      <td>155066</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Hip-Hop Beats, Hip-Hop]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7994 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID    Genre                     Genres_all\n",
       "0          2  Hip-Hop                      [Hip-Hop]\n",
       "1          5  Hip-Hop                      [Hip-Hop]\n",
       "2         10      Pop                          [Pop]\n",
       "3        140     Folk                         [Folk]\n",
       "4        141     Folk                         [Folk]\n",
       "...      ...      ...                            ...\n",
       "7989  154308  Hip-Hop  [Hip-Hop Beats, Rap, Hip-Hop]\n",
       "7990  154309  Hip-Hop  [Hip-Hop Beats, Rap, Hip-Hop]\n",
       "7991  154413      Pop        [Pop, Experimental Pop]\n",
       "7992  154414      Pop        [Pop, Experimental Pop]\n",
       "7993  155066  Hip-Hop       [Hip-Hop Beats, Hip-Hop]\n",
       "\n",
       "[7994 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hip-Hop', 'Pop', 'Folk', 'Experimental', 'Rock', 'International',\n",
       "       'Electronic', 'Instrumental'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_genres = data['Genre'].unique()\n",
    "unique_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe and CSV to work with when training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "spectrogram_dir = '/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/spectrograms' \n",
    "spectrogram_files = os.listdir(spectrogram_dir)\n",
    "\n",
    "'''\n",
    "spectrograms = []\n",
    "for file in spectrogram_files:\n",
    "    if file.endswith('.png'):\n",
    "        image = Image.open(os.path.join(spectrogram_dir, file))\n",
    "        image_array = np.array(image)\n",
    "        spectrograms.append(image_array)\n",
    "'''\n",
    "y = data['Genre'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7518\n",
      "7518\n",
      "7518\n"
     ]
    }
   ],
   "source": [
    "print(len([f'/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/spectrograms/{spectrogram_files[i]}' for i in range(len(spectrogram_files))]))\n",
    "print(len(y))\n",
    "print(len(data['Genres_all'].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the audio files could not be converted into spectograms due to corrupted data, empty files... This dataset reduction comes from this corrupted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'image_paths': [f'/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/spectrograms/{spectrogram_files[i]}' for i in range(len(spectrogram_files))],\n",
    "    'labels': y,\n",
    "    'other_labels': data['Genres_all'].values\n",
    "})\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv('/home/xnmaster/TestMachine/deep-learning-project-2024-ai_nndl_g9/MY_DATA.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
